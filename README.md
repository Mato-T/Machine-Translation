# Project Description
## Introduction
- The Transformer architecture introduced in the paper "Attention Is All You Need" one was major leap forwards in the development of more sophisticated Natural Language Models and it paved the way for popular models like GPT and BERT. As the time of this writing, major tech-companies like Microsoft, Google, OpenAI, Meta, etc. announce their own models that were only possible due to the development of Transformers.
- For this reason, I decided to take a deeper look into the architecture behind this revolutionary model and assessed machine translation as a fitting project. The dataset consists of French sentences and their Enlgish translation (or vice versa) and can be found on Kaggle using this URL: https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset
- Please note that the following information should be read along with the Python code in this repository and requires a more profound understanding of neural networks, including the Sequence-to-Sequence model and the Attention mechanism. Each concept described in this documentation is explained in more detail in the Concept.md file. For more information on the cross-entropy loss, check out the Concept.md file in the Fraud-Detection repository.
## Preprocessing
- 
